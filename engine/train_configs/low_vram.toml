# Genesis Training Config: Low VRAM
# Optimized for 4-6 GB VRAM (e.g., GTX 1660, RTX 3050, Quadro T2000)

[training]
batch_size = 1
gradient_accumulation_steps = 32
steps = 1000
learning_rate = 3e-5
precision = "fp16"
compile = false
flash_attention = false
gradient_checkpointing = true

[parallelism]
fsdp = false

[model]
name = "micro-llama-124m"
vocab_size = 8000
n_layers = 12
dim = 768
n_heads = 12
intermediate_size = 3072
max_seq_len = 1024

[checkpoint]
interval = 250
dir = "../checkpoints"
export_format = "safetensors"

[masking]
enabled = false  # Set to true to enable weighted masking
strategy = "weighted"
base_probability = 0.4  # Base masking probability (multiplied by difficulty weights)

