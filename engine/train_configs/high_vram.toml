# Genesis Training Config: High VRAM
# Optimized for 12+ GB VRAM (e.g., RTX 3080, 4070, A5000+)

[training]
batch_size = 4
gradient_accumulation_steps = 4
steps = 2000
learning_rate = 5e-5
precision = "bf16"
compile = false # Disabled by default for immediate feedback
flash_attention = true

[parallelism]
fsdp = false # Disabled for local single-GPU runs

[model]
name = "micro-llama-124m"
vocab_size = 8000
n_layers = 12
dim = 768
n_heads = 12
intermediate_size = 3072
max_seq_len = 1024

[checkpoint]
interval = 500
dir = "checkpoints"
export_format = "safetensors"

[masking]
enabled = false  # Set to true to enable weighted masking
strategy = "weighted"
base_probability = 0.4  # Base masking probability (multiplied by difficulty weights)

