# ============================================================================
# GENESIS ARBITER - Global Project Configuration
# ============================================================================
# This file is the single source of truth for the entire repository.
# Prioritized by Research Workflow: Inference -> Training -> Systems.

[inference]
# Device: "auto" (detects CUDA), "cuda", or "cpu"
device = "cuda"

# Generation Control
max_tokens = 100
temperature = 0.8
top_k = 40
top_p = 0.9
repetition_penalty = 1.1

# UX Settings
stream_output = true
show_probs = false
stop_sequences = ["\n\n\n"]

[training]
# Default training preset ("microscope", "deep_narrow_40", "deep_narrow_48")
mode = "microscope"

# Optimization Parameters
learning_rate = 1e-4
weight_decay = 0.08
max_steps = 30000
batch_size = 1024
grad_accum_steps = 1
lr_schedule = "cosine"
warmup_steps = 2000
min_lr_ratio = 0.1

# Checkpoint Control
save_interval = 2000
resume = true
autoresume_best = true

[model]
# Default architecture (if not using a preset mode)
vocab_size = "auto"
dim = 512
n_layers = 12
n_heads = 8
intermediate_size = 2048
norm_type = "deepnorm"
max_seq_len = 512

[data]
# Source directories (root-relative)
bible_dir = "Bible"

# Generated artifacts
tokenizer_path = "data/genesis_char_tokenizer.json"
cache_path = "data/genesis_data_cache.pt"

# Sampling settings
max_seq_len = 512
target_languages = []  # Empty for all languages

[evaluation]
# Operational Intervals (steps)
val_interval = 500
log_interval = 100
eval_interval = 1000

# Probe depth
eval_recon_samples = 50
eval_aux_samples = 20

# Features
detect_grokking = true

[system]
# Precision: "fp32", "fp16", "bf16", "int8", "int4"
precision = "bf16"

# Performance Flags
compile_model = false
gradient_checkpointing = true
cpu_data = false
verbose_logging = true

# Environment
use_libuv = "0"

[tasks]
# Multi-task training distribution (sums to 1.0)
lm = 0.70
coherence = 0.15
cross_ref = 0.075
paraphrase = 0.075

[sweep]
# Parameter space for hyperparameter exploration
# Values must be lists
n_layers = [12, 16, 24]
dim = [512, 768]
learning_rate = [1e-4, 3e-4]
weight_decay = [0.05, 0.1]
strategy = "grid"  # Options: "grid", "random"
num_trials = 10
output_dir = "./sweep_runs"
