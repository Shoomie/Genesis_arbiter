# ============================================================================
# GENESIS ARBITER - Global Project Configuration
# ============================================================================
# This file is the single source of truth for the entire repository.
# Prioritized by Research Workflow: Inference -> Training -> Systems.

[inference]
# Device: "auto" (detects CUDA), "cuda", or "cpu"
device = "cuda"

# Generation Control
max_tokens = 100
temperature = 0.7
top_k = 40
top_p = 0.9
repetition_penalty = 1.5

# UX Settings
stream_output = true
show_probs = false
stop_sequences = ["\n\n\n"]

[training]
# Default training preset ("microscope", "deep_narrow_40", "deep_narrow_48")
mode = "microscope"

# Optimization Parameters
learning_rate = 5e-4
weight_decay = 1
max_steps = 200000
batch_size = 32
grad_accum_steps = 1
lr_schedule = "cosine"
warmup_steps = 1000
min_lr_ratio = 0.05

# Checkpoint Control
save_interval = 500
resume = true
autoresume_best = true

[model]
# Default architecture (if not using a preset mode)
vocab_size = "auto"
dim = 256
n_layers = 12
n_heads = 4
intermediate_size = 1024
norm_type = "deepnorm"
max_seq_len = 2048

[data]
# Source directories (root-relative)
bible_dir = "../Bible"

# Generated artifacts
tokenizer_path = "data/genesis_char_tokenizer.json"
cache_path = "data/genesis_data_cache.pt"

# Sampling settings
max_seq_len = 2048
target_languages = []  # Empty for all languages

[evaluation]
# Operational Toggles
enable_validation = false
enable_extensive_eval = false

# Operational Intervals (steps)
val_interval = 750
log_interval = 10
eval_interval = 500

# Probe depth
eval_recon_samples = 50
eval_aux_samples = 20

# Features
detect_grokking = true

[system]
# Precision: "fp32", "fp16", "bf16", "int8", "int4"
precision = "bf16"

# Performance Flags
compile_model = false
gradient_checkpointing = true
cpu_data = false
verbose_logging = true

# Environment
use_libuv = "0"

[tasks]
# Multi-task training distribution (sums to 1.0)
lm = 1.0

[sweep]
# Parameter space for hyperparameter exploration
# Values must be lists
n_layers = [12, 16, 24]
dim = [512, 768]
learning_rate = [1e-4, 3e-4]
weight_decay = [0.05, 0.1]
strategy = "grid"  # Options: "grid", "random"
num_trials = 10
output_dir = "./sweep_runs"
