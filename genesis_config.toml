# ============================================================================
# GENESIS ARBITER - Global Project Configuration
# ============================================================================
# This file is the single source of truth for the entire repository.
# Prioritized by Research Workflow: Inference -> Training -> Systems.

[inference]
# Device: "auto" (detects CUDA), "cuda", or "cpu"
device = "cuda"

# Generation Control
max_tokens = 200
temperature = 0.7
top_k = 40
top_p = 0.9
repetition_penalty = 1

# UX Settings
stream_output = true
show_probs = false
stop_sequences = ["\n\n\n"]

[training]
# Default training preset ("microscope", "deep_narrow_40", "deep_narrow_48")
mode = "microscope"

# Optimization Parameters
learning_rate = 3e-5
weight_decay = 0.01
max_steps = 10000
batch_size = 8
grad_accum_steps = 4

# Automated Plateau Recovery (Suggestion 3)
plateau_lr_drop = 0.5     # Drop LR by 50% on plateau
plateau_lr_patience = 1000 # Wait N steps of stagnation before dropping
lr_schedule = "cosine"
warmup_steps = 100
min_lr_ratio = 0.05

# Checkpoint Control
save_interval = 100
resume = true
autoresume_best = true

# Dynamic WWM Trigger (Semantic Grokking)
wwm_trigger_steps = 500  # Initial steps to wait
wwm_window = 100         # Check improvement every N steps
wwm_threshold = 0.02    # Trigger if improvement < 2.0%
wwm_mask_prob = 0.15      # Percentage of words to mask in Phase 2
wwm_ramp_steps = 1000     # Linearly ramp from 5% to 15% over N steps

# Phase 3: Span Masking (Structural Grokking)
span_trigger_steps = 1000 # Wait N steps after WWM triggers
span_window = 100         # Check improvement every N steps
span_threshold = 0.01    # Trigger if improvement < 1.0%
span_mask_prob = 0.10     # Percentage of words to mask in Phase 3
span_ramp_steps = 500     # Linearly ramp to final density over N steps
span_min_len = 3
span_max_len = 5

[model]
# Default architecture (if not using a preset mode)
vocab_size = "auto"
dim = 192
n_layers = 16
n_heads = 6
intermediate_size = 768
norm_type = "deepnorm"
max_seq_len = 8192

[data]
# Source directories (root-relative)
bible_dir = "../Bible"

# Generated artifacts
cache_path = "data/genesis_data_cache.pt"

# Sampling settings
max_seq_len = 8192
target_languages = []  # Empty for all languages

[evaluation]
# Operational Toggles
enable_validation = false
enable_extensive_eval = false

# Operational Intervals (steps)
val_interval = 750
log_interval = 10
eval_interval = 500

# Probe depth
eval_recon_samples = 50
eval_aux_samples = 20

# Features
detect_grokking = true

[system]
# Precision: "fp32", "fp16", "bf16", "int8", "int4"
precision = "bf16"

# Performance Flags
compile_model = false
gradient_checkpointing = false
cpu_data = false
verbose_logging = true

# Environment
use_libuv = "0"

[tasks]
# Training distribution (sums to 1.0)
lm = 1.0

