# ============================================================================
# GENESIS ARBITER - Global Project Configuration
# ============================================================================
# This file is the single source of truth for the entire repository.
# Prioritized by Research Workflow: Inference -> Training -> Systems.

[inference]
# Device: "auto" (detects CUDA), "cuda", or "cpu"
device = "cuda"

# Generation Control
max_tokens = 200
temperature = 0
top_k = 40
top_p = 0.9
repetition_penalty = 1.3

# UX Settings
stream_output = true
show_probs = false
stop_sequences = ["\n\n\n"]

[training]
# Default training preset ("microscope", "deep_narrow_40", "deep_narrow_48")
mode = "microscope"

# Optimization Parameters
learning_rate = 5e-5
weight_decay = 0.01
max_steps = 10000
batch_size = 8
grad_accum_steps = 8
lr_schedule = "cosine"
warmup_steps = 100
min_lr_ratio = 0.05

# Checkpoint Control
save_interval = 100
resume = true
autoresume_best = true

# Dynamic WWM Trigger (Semantic Grokking)
wwm_trigger_steps = 500  # Initial steps to wait
wwm_window = 100         # Check improvement every N steps
wwm_threshold = 0.005     # Trigger if improvement < 0.5%

# Phase 3: Span Masking (Structural Grokking)
span_trigger_steps = 500 # Wait at least N steps AFTER WWM activation
span_window = 100         # Check improvement every N steps
span_threshold = 0.002    # Trigger if improvement < 0.2%
span_min_len = 3
span_max_len = 5

[model]
# Default architecture (if not using a preset mode)
vocab_size = "auto"
dim = 192
n_layers = 12
n_heads = 6
intermediate_size = 768
norm_type = "deepnorm"
max_seq_len = 8192

[data]
# Source directories (root-relative)
bible_dir = "../Bible"

# Generated artifacts
tokenizer_path = "data/genesis_char_tokenizer.json"
cache_path = "data/genesis_data_cache.pt"

# Sampling settings
max_seq_len = 8192
target_languages = []  # Empty for all languages

[evaluation]
# Operational Toggles
enable_validation = false
enable_extensive_eval = false

# Operational Intervals (steps)
val_interval = 750
log_interval = 10
eval_interval = 500

# Probe depth
eval_recon_samples = 50
eval_aux_samples = 20

# Features
detect_grokking = true

[system]
# Precision: "fp32", "fp16", "bf16", "int8", "int4"
precision = "bf16"

# Performance Flags
compile_model = false
gradient_checkpointing = false
cpu_data = false
verbose_logging = true

# Environment
use_libuv = "0"

[tasks]
# Training distribution (sums to 1.0)
lm = 1.0

[sweep]
# Parameter space for hyperparameter exploration
# Values must be lists
n_layers = [12, 16, 24]
dim = [512, 768]
learning_rate = [1e-4, 3e-4]
weight_decay = [0.05, 0.1]
strategy = "grid"  # Options: "grid", "random"
num_trials = 10
output_dir = "./sweep_runs"
