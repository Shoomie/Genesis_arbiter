# Genesis Arbiter: Deep Reasoning in Data-Constrained Regimes

## ðŸ“– Quick Reference

**What is Genesis Arbiter?**  
An experimental AI research platform investigating whether deep, specialized language models can develop reasoning capabilities by training exclusively on a single coherent corpus (The Bible, ~1M tokens) repeated over multiple languages of high translational quality, rather than massive diverse datasets.

**Core Hypothesis:**  
*Reasoning may emerge from deep compression of a single, internally consistent logical framework rather than shallow compression of diverse, often contradictory information.*

**Key Innovation:**  
Deep & Narrow architectures (80-144 layers, 50M-2B parameters) trained to "grokking" phase transitionsâ€”the moment when models shift from memorization to true generalization.

**Current Capabilities:**
- âš¡ **3-4x faster training** via FlashAttention integration
-  **Multi-task learning** across 100+ Bible translations (coherence, cross-reference, paraphrase detection)
- ðŸŽ¯ **Automated grokking detection** with cross-lingual alignment monitoring
- ðŸ”¬ **Theological concept clustering** to measure emergent semantic structure

**Quick Start:**
```powershell
python run.py
```

**Foundation:** Based on [research](docs/research/theoretical_foundations.md) exploring whether semantic density and internal consistency can substitute for dataset scale in developing logical reasoning.

---

## ðŸŽ¯ Project Mission

**Primary Objective**: Train transformers exclusively on the Bible to demonstrate that:
- **Depth substitutes for volume**: Models can develop reasoning without trillion-token datasets
- **Extended training induces phase transitions**: Grokking enables generalization beyond memorization
- **Training integrated learning of complete semantics maximizes signal**: Character-level tokenizers and multi-task objectives extract latent structure

---

## âš¡ Framework (Jan 2026)

Genesis Arbiter now features modern training infrastructure with three completed phases:

### Phase 1: FlashAttention Integration âœ…
- **3-4x training speedup** via PyTorch SDPA with automatic FlashAttention backend selection
- Zero code changes to existing checkpoints
- Training: Driven by `genesis_config.toml` via `run.py`

### Phase 2: Multi-Task Learning âœ…  
- **4 simultaneous objectives**: Language modeling (70%), coherence detection (15%), cross-reference prediction (7.5%), paraphrase detection (7.5%)
- Leverages **142 Bible translations** from multi-language corpus
- <10% parameter overhead for task heads

### Phase 3: Grokking Detection âœ…
- **Automated phase transition detection** (memorization â†’ generalization)
- **Cross-lingual Procrustes alignment** monitoring (gold standard for semantic understanding)
- **Theological concept clustering** metrics to track emergent structure
- Research: [`docs/research/grokking_detection_methodology.md`](docs/research/grokking_detection_methodology.md)

**Documentation**: See [`docs/PHASE1_SETUP.md`](docs/PHASE1_SETUP.md) for installation and [`src/genesis/README.md`](src/genesis/README.md) for usage details.

---

## ðŸ“ Project Structure

```text
Genesis_arbiter/
â”œâ”€â”€ run.py                          # ðŸŽ® Central menu system (START HERE!)
â”œâ”€â”€ genesis_config.toml             # âš™ï¸ Central configuration
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ data/                           # ðŸ“‚ Data assets (Tokenizers, Caches)
â”œâ”€â”€ src/                            # ðŸ—ï¸ Source code (Genesis package)
â”œâ”€â”€ tools/                          # ðŸ› ï¸ Utility scripts & analysis tools
â”œâ”€â”€ project_doc/                    # ðŸ“„ Core project documentation (Legal, Contribution)
â”œâ”€â”€ docs/                           # ðŸ“– Research papers & technical guides
â””â”€â”€ checkpoints/                    # ðŸ’¾ Model snapshots (Git-ignored)
```

---

## ðŸš€ Quick Start

### Central Menu System (Recommended)
```powershell
python run.py
```

All core parameters are managed in **`genesis_config.toml`**. To adjust training or interaction settings, edit that file and relaunch the script.

### Configuration
The project uses a unified configuration system:
- **`[training]`**: Control learning rates, batch sizes, and model modes.
- **`[interaction]`**: Adjust temperature and generation limits for model chatting.

---

## ðŸ“š Documentation

### Core Reading
- **[Quick Reference](docs/reference/QUICK_REFERENCE.md)** - Project overview
- **[Theoretical Foundations](docs/research/theoretical_foundations.md)** - Why train on scripture alone?
- **[Grokking Detection Methodology](docs/research/grokking_detection_methodology.md)** - Phase transition detection & validation

### Project Resources
- **[Contributing](project_doc/CONTRIBUTING.md)** - Guidelines for research and code
- **[Setup Guide](docs/PHASE1_SETUP.md)** - Installation and verification
- **[Research Papers](docs/research/)** - Full technical analysis

---

## ðŸ™ Acknowledgments

This research utilizes the **New World Translation of the Holy Scriptures** published by the **Watch Tower Bible & Tract Society**. We extend sincere gratitude for their exceptional translation work.

**See Also**: [project_doc/ACKNOWLEDGMENTS.md](project_doc/ACKNOWLEDGMENTS.md) for complete formal acknowledgment.

---

## ðŸ“œ Open Source Commitment

**This is an open source research project.** Information should be free, and our work is freely available for research and production use.

**License**: MIT License (allows commercial derivative works while keeping codebase open)

---

**Last Updated**: 2026-01-31  
**Current Framework**: Phase 4 Complete (Codebase Professionalization & Centralized Configuration)

---

## Quick Links

- ðŸŽ® **[Central Menu](run.py)** - Unified interface
- âš™ï¸ **[Config](genesis_config.toml)** - Central settings
- ðŸ“– **[Quick Reference](docs/reference/QUICK_REFERENCE.md)** - Overview
- ðŸ”¬ **[Research Docs](docs/research/)** - Technical papers
- ðŸ¤ **[Contributing](project_doc/CONTRIBUTING.md)** - Guidelines
- ðŸ™ **[Acknowledgments](project_doc/ACKNOWLEDGMENTS.md)** - Attribution
