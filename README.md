# Genesis Arbiter: Deep Reasoning in Data-Constrained Regimes

## ğŸ“– Quick Reference

**What is Genesis Arbiter?**  
An experimental AI research platform investigating whether deep, specialized language models can develop reasoning capabilities by training exclusively on a single coherent corpus (The Bible, ~1M tokens) repeated over multiple languages of high translational quality, rather than massive diverse datasets.

**Core Hypothesis:**  
*Reasoning may emerge from deep compression of a single, internally consistent logical framework rather than shallow compression of diverse, often contradictory information.*

**Current Capabilities:**
- âš¡ **High-Efficiency Training**: FlashAttention 2.0 + GPU-resident data loading for maximum throughput.
- ğŸ§¬ **WWM & Span Curriculum**: Staged masking (Single â†’ WWM â†’ Span) with linear difficulty ramping.
- ğŸ“‰ **Automated Plateau Recovery**: "LR Stun" mechanism to break training stagnation during phases.
- ğŸ“Š **Research-Grade Telemetry**: Multi-lingual metrics, stagnation tracking, and phase-anchored improvement stats.

**Quick Start:**
```powershell
python run.py
```

---

## ğŸ¯ Project Mission

**Primary Objective**: Train transformers exclusively on the Bible to demonstrate that:
- **Depth substitutes for volume**: Models can develop reasoning without trillion-token datasets
- **Extended training induces phase transitions**: Grokking enables generalization beyond memorization
- **Training integrated learning of complete semantics maximizes signal**: Character-level tokenizers and multi-task objectives extract latent structure

---

**Documentation**: See [`src/genesis/README.md`](src/genesis/README.md) for usage details.

---

## ğŸ“ Project Structure

```text
Genesis_arbiter/
â”œâ”€â”€ run.py                          # ğŸ® Central menu system (START HERE!)
â”œâ”€â”€ genesis_config.toml             # âš™ï¸ Central configuration
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ data/                           # ğŸ“‚ Data assets (Tokenizers, Caches)
â”œâ”€â”€ src/                            # ğŸ—ï¸ Source code (Genesis package)
â”œâ”€â”€ tools/                          # ğŸ› ï¸ Utility scripts & analysis tools
â”œâ”€â”€ project_doc/                    # ğŸ“„ Core project documentation (Legal, Contribution)
â”œâ”€â”€ docs/                           # ğŸ“– Research papers & technical guides
â””â”€â”€ checkpoints/                    # ğŸ’¾ Model snapshots (Git-ignored)
```

---

## ğŸš€ Quick Start

### Central Menu System (Recommended)
```powershell
python run.py
```

All core parameters are managed in **`genesis_config.toml`**. To adjust training or interaction settings, edit that file and relaunch the script.

### Configuration
The project uses a unified configuration system:
- **`[training]`**: Control learning rates, batch sizes, and model modes.
- **`[interaction]`**: Adjust temperature and generation limits for model chatting.

---

## ğŸ“š Documentation

### Core Reading
- **[Theoretical Foundations](docs/reference/theoretical_foundations.md)** - Why train on scripture alone?

### Project Resources
- **[Contributing](project_doc/CONTRIBUTING.md)** - Guidelines for research and code
- **[Research Papers](docs/research/)** - Collection of various research done during the project

---

## ğŸ™ Acknowledgments

This research utilizes the **New World Translation of the Holy Scriptures** published by the **Watch Tower Bible & Tract Society**. We extend sincere gratitude for their exceptional translation work.

**See Also**: [project_doc/ACKNOWLEDGMENTS.md](project_doc/ACKNOWLEDGMENTS.md) for complete formal acknowledgment.

---

## ğŸ“œ Open Source Commitment

**This is an open source research project.** Information should be free, and our work is freely available for research and production use.

**License**: MIT License (allows commercial derivative works while keeping codebase open)

---

**Last Updated**: 2026-02-04

---

## Quick Links

- ğŸ® **[Central Menu](run.py)** - Unified interface
- âš™ï¸ **[Config](genesis_config.toml)** - Central settings
- ğŸ”¬ **[Research Docs](docs/research/)** - Technical papers
- ğŸ¤ **[Contributing](project_doc/CONTRIBUTING.md)** - Guidelines
- ğŸ™ **[Acknowledgments](project_doc/ACKNOWLEDGMENTS.md)** - Attribution
